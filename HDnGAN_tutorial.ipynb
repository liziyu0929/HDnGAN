{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HDnGAN_tutorial.ipynb","provenance":[],"mount_file_id":"1NkEbrHgFhgAsCRBc1hnckAtQlG1sXOAJ","authorship_tag":"ABX9TyNL68AneCgn8/8ZxGjAf5Ct"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"6Qeq86WVIlFd"},"source":["# s_HDnGAN_trainHDnGAN.py\n","#\n","#   A script for training HDnGAN for high-fidelity MRI denoising.\n","#\n","#   Source code:\n","#   https://github.com/liziyu0929/HDnGAN/blob/main/s_HDnGAN_trainCNN.py\n","#\n","#   Reference:\n","#   [1] Li, Z., Tian, Q., Ngamsombat, C., Cartmell, S., Conklin, J., Gonçalves Filho, A. L. M., ... & Huang, S. Y. (2021). \n","#       High-fidelity fast volumetric brain MRI using synergistic wave-controlled aliasing in parallel imaging and a hybrid \n","#       denoising generative adversarial network. bioRxiv: https://www.biorxiv.org/content/10.1101/2021.01.07.425779v2.abstract. \n","#        (Submitted to Medical Physics).\n","#\n","#   [2] Li, Z. Oral Presentation. The 2021 Annual Scientific Meeting of ISMRM. Video link: \n","#       https://cds.ismrm.org/protected/21MPresentations/videos//0390.htm. (Magna Cum Laude Merit Award).\n","#\n","# (c) Ziyu Li, Qiyuan Tian, 2021\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3hdRAGeDMO0x","executionInfo":{"status":"ok","timestamp":1636907948572,"user_tz":0,"elapsed":56725,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"cbc8cd7d-2bd8-4879-92c5-1c737635d0fc"},"source":["!pip install tensorflow-gpu==2.1.0\n","!pip install keras==2.3.1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorflow-gpu==2.1.0\n","  Downloading tensorflow_gpu-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n","\u001b[K     |████████████████████████████████| 421.8 MB 5.6 kB/s \n","\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n","  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 41.0 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 487 kB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.1.2)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.4.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.19.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (3.17.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.41.1)\n","Collecting tensorboard<2.2.0,>=2.1.0\n","  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 46.0 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.37.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (3.3.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.13.3)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.2.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (0.12.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.1.0) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.1.0) (3.1.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.4.6)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.3.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (4.8.2)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (1.24.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.1.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.1.0) (1.5.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu==2.1.0) (3.10.0.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=8c7679e24e6d1f8eeb819f25c52368c1355140fc84669c2fc70e9998a7c695e0\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.1.1 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 2.1.0 which is incompatible.\n","tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 3.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.0.8)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.3.1 which is incompatible.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.1.1 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n","Successfully installed keras-2.3.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZW10vll2UmrH","executionInfo":{"status":"ok","timestamp":1636908285663,"user_tz":0,"elapsed":747,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"a7a766ef-e4dd-4f7a-a07c-3d482ab4dabe"},"source":["%cd drive/MyDrive/HDnGAN\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/HDnGAN\n","cnn_models.py\t  HDnGAN\t\t README.md\n","cnn_utils.py\t  HDnGAN_tutorial.ipynb  s_HDnGAN_trainCNN.py\n","example_hcp_data  LICENSE.md\t\t SpectralNormalizationKeras.py\n","fig\t\t  loss\n","generator\t  __pycache__\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rowZXoaL5DL","executionInfo":{"status":"ok","timestamp":1636908293354,"user_tz":0,"elapsed":4292,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"7176aed0-a39e-4960-822f-e5c983f24649"},"source":["# %% load moduals\n","import os\n","import glob\n","import scipy.io as sio\n","import numpy as np\n","import nibabel as nb\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","from keras.optimizers import Adam\n","\n","from cnn_models import generator_3d_model, discriminator_2d_model, gan_hybrid_model\n","import cnn_utils as utils"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]}]},{"cell_type":"code","metadata":{"id":"36RVK1YHIqd_"},"source":["# for compatibility\n","from tensorflow.compat.v1 import ConfigProto\n","from tensorflow.compat.v1 import InteractiveSession\n","from tensorflow.compat.v1 import GPUOptions\n","\n","gpu_options = GPUOptions(per_process_gpu_memory_fraction=0.9)\n","config = ConfigProto(gpu_options=gpu_options)\n","config.gpu_options.allow_growth = True\n","session = InteractiveSession(config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BsBRGt9S6aU","executionInfo":{"status":"ok","timestamp":1636908303503,"user_tz":0,"elapsed":841,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"e27fdd34-f9ff-416b-a507-ccee0b625264"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"id":"DJ4y18BmMMYb"},"source":["# %% set up path and parameters\n","dpRoot = os.path.dirname(os.path.abspath('s_HDnGAN_trainCNN.py'))\n","os.chdir(dpRoot)\n","\n","block_size = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SFDG6AARpos","executionInfo":{"status":"ok","timestamp":1636908343179,"user_tz":0,"elapsed":33618,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"027f4377-83dd-40e3-efb2-ee77ee6af9ca"},"source":["# %% subjects\n","subjects = sorted(glob.glob(os.path.join(dpRoot, 'example_hcp_data', 'mwu*')))\n","\n","# %% load data \n","noisy_block_train = np.zeros(1)\n","noisy_block_test = np.zeros(1)\n","\n","clean_block_train = np.zeros(1)\n","clean_block_test = np.zeros(1)\n","\n","bmask_block_train = np.zeros(1)\n","bmask_block_test = np.zeros(1)\n","\n","for ii in range(len(subjects)):\n","    sj = os.path.basename(subjects[ii])\n","    \n","    print(sj)\n","    dpSub = os.path.join(dpRoot, 'example_hcp_data', sj)\n","    \n","    fpNoisy = os.path.join(dpSub, sj + '_t1w_sim0.5.nii.gz')\n","    fpClean = os.path.join(dpSub, sj + '_t1w.nii.gz')\n","    fpBmask = os.path.join(dpSub, 'brainmask_fs_dil2.nii.gz')\n","\n","    noisy_struct = nb.load(fpNoisy)\n","    clean_struct = nb.load(fpClean)\n","    bmask_struct = nb.load(fpBmask)\n","\n","    noisy = np.squeeze(np.array(noisy_struct.dataobj))\n","    clean = np.squeeze(np.array(clean_struct.dataobj))\n","    bmask = np.squeeze(np.array(bmask_struct.dataobj))\n","    \n","    noisy = np.expand_dims(noisy, -1)\n","    clean = np.expand_dims(clean, -1)\n","    bmask = np.expand_dims(bmask, -1)\n","    \n","    # normalize\n","    clean_norm, noisy_norm = utils.normalize_image(clean, noisy, bmask)\n","    \n","    # get block\n","    [ind_block, ind_brain] = utils.block_ind(bmask, block_size, 1)\n","    noisy_block = utils.extract_block(noisy_norm, ind_block)\n","    clean_block = utils.extract_block(clean_norm, ind_block)\n","    bmask_block = utils.extract_block(bmask, ind_block)\n","    \n","    if np.mod(ii, 3) == 0:\n","        print('validation & evalution subject')\n","        # here for demonstration purpose we use the same subject for validation and evalution \n","        if noisy_block_test.any():\n","            noisy_block_test = np.concatenate((noisy_block_test, noisy_block), axis=0)\n","            clean_block_test = np.concatenate((clean_block_test, clean_block), axis=0)\n","            bmask_block_test = np.concatenate((bmask_block_test, bmask_block), axis=0)\n","        else:\n","            noisy_block_test = noisy_block\n","            clean_block_test = clean_block\n","            bmask_block_test = bmask_block\n","            noisy_block_apply = noisy_block # for apply\n","            bmask_block_apply = bmask_block\n","            dpSub_apply = dpSub\n","            bmask_apply = bmask\n","            ind_apply = ind_block\n","    else:\n","        if noisy_block_train.any():\n","            noisy_block_train = np.concatenate((noisy_block_train, noisy_block), axis=0)\n","            clean_block_train = np.concatenate((clean_block_train, clean_block), axis=0)\n","            bmask_block_train = np.concatenate((bmask_block_train, bmask_block), axis=0)\n","        else:\n","            noisy_block_train = noisy_block\n","            clean_block_train = clean_block            \n","            bmask_block_train = bmask_block\n","            \n","\n","# setup data\n","clean_output_train = np.concatenate((clean_block_train, bmask_block_train), axis=-1)\n","clean_output_test = np.concatenate((clean_block_test, bmask_block_test), axis=-1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mwu100307\n","validation & evalution subject\n","mwu102311\n","mwu102816\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsaOclvPR32P","executionInfo":{"status":"ok","timestamp":1636908371467,"user_tz":0,"elapsed":2525,"user":{"displayName":"AINI Software","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11137961844590960622"}},"outputId":"1eb78ed1-3703-43b4-904d-bbfc515f79d5"},"source":["# %% set up models\n","input_ch_g = 1\n","input_ch_d = 1\n","\n","model_generator = generator_3d_model(input_ch_g)\n","model_generator.summary()\n","model_discriminator = discriminator_2d_model(block_size, input_ch_d)\n","model_discriminator.summary()\n","model_discriminator.trainable = False\n","model_gan = gan_hybrid_model(block_size, input_ch_g, input_ch_d, model_generator, model_discriminator)\n","model_gan.summary()\n","\n","# set up optimizer\n","opt_g = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n","opt_d = Adam(lr=0.0003, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n","\n","# compile models\n","model_generator.compile(loss = utils.mean_squared_error_weighted, optimizer = opt_g)\n","model_discriminator.trainable = True\n","model_discriminator.compile(loss = 'binary_crossentropy', optimizer = opt_d)\n","model_discriminator.trainable = False\n","loss = [utils.mean_squared_error_weighted, 'binary_crossentropy']\n","loss_weights = [1, 1e-3]\n","model_gan.compile(optimizer = opt_g, loss = loss, loss_weights=loss_weights)\n","model_discriminator.trainable = True"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, None, N 0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None, None, N 0                                            \n","__________________________________________________________________________________________________\n","multiply_1 (Multiply)           (None, None, None, N 0           input_1[0][0]                    \n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv3d_1 (Conv3D)               (None, None, None, N 1792        multiply_1[0][0]                 \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, None, None, N 0           conv3d_1[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_2 (Conv3D)               (None, None, None, N 110656      activation_1[0][0]               \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, None, None, N 0           conv3d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_3 (Conv3D)               (None, None, None, N 110656      activation_2[0][0]               \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, None, None, N 0           conv3d_3[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_4 (Conv3D)               (None, None, None, N 110656      activation_3[0][0]               \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, None, None, N 0           conv3d_4[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_5 (Conv3D)               (None, None, None, N 110656      activation_4[0][0]               \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, None, None, N 0           conv3d_5[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_6 (Conv3D)               (None, None, None, N 110656      activation_5[0][0]               \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, None, None, N 0           conv3d_6[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_7 (Conv3D)               (None, None, None, N 110656      activation_6[0][0]               \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, None, None, N 0           conv3d_7[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_8 (Conv3D)               (None, None, None, N 110656      activation_7[0][0]               \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, None, None, N 0           conv3d_8[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_9 (Conv3D)               (None, None, None, N 110656      activation_8[0][0]               \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, None, None, N 0           conv3d_9[0][0]                   \n","__________________________________________________________________________________________________\n","conv3d_10 (Conv3D)              (None, None, None, N 110656      activation_9[0][0]               \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, None, None, N 0           conv3d_10[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, None, None, N 0           activation_8[0][0]               \n","                                                                 activation_10[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_11 (Conv3D)              (None, None, None, N 221248      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, None, None, N 0           conv3d_11[0][0]                  \n","__________________________________________________________________________________________________\n","conv3d_12 (Conv3D)              (None, None, None, N 110656      activation_11[0][0]              \n","__________________________________________________________________________________________________\n","activation_12 (Activation)      (None, None, None, N 0           conv3d_12[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, None, None, N 0           activation_6[0][0]               \n","                                                                 activation_12[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_13 (Conv3D)              (None, None, None, N 221248      concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","activation_13 (Activation)      (None, None, None, N 0           conv3d_13[0][0]                  \n","__________________________________________________________________________________________________\n","conv3d_14 (Conv3D)              (None, None, None, N 110656      activation_13[0][0]              \n","__________________________________________________________________________________________________\n","activation_14 (Activation)      (None, None, None, N 0           conv3d_14[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_3 (Concatenate)     (None, None, None, N 0           activation_4[0][0]               \n","                                                                 activation_14[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_15 (Conv3D)              (None, None, None, N 221248      concatenate_3[0][0]              \n","__________________________________________________________________________________________________\n","activation_15 (Activation)      (None, None, None, N 0           conv3d_15[0][0]                  \n","__________________________________________________________________________________________________\n","conv3d_16 (Conv3D)              (None, None, None, N 110656      activation_15[0][0]              \n","__________________________________________________________________________________________________\n","activation_16 (Activation)      (None, None, None, N 0           conv3d_16[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_4 (Concatenate)     (None, None, None, N 0           activation_2[0][0]               \n","                                                                 activation_16[0][0]              \n","__________________________________________________________________________________________________\n","conv3d_17 (Conv3D)              (None, None, None, N 221248      concatenate_4[0][0]              \n","__________________________________________________________________________________________________\n","activation_17 (Activation)      (None, None, None, N 0           conv3d_17[0][0]                  \n","__________________________________________________________________________________________________\n","conv3d_18 (Conv3D)              (None, None, None, N 110656      activation_17[0][0]              \n","__________________________________________________________________________________________________\n","activation_18 (Activation)      (None, None, None, N 0           conv3d_18[0][0]                  \n","__________________________________________________________________________________________________\n","conv3d_19 (Conv3D)              (None, None, None, N 1729        activation_18[0][0]              \n","__________________________________________________________________________________________________\n","modified_unet3d_add_residual (A (None, None, None, N 0           multiply_1[0][0]                 \n","                                                                 conv3d_19[0][0]                  \n","__________________________________________________________________________________________________\n","multiply_2 (Multiply)           (None, None, None, N 0           modified_unet3d_add_residual[0][0\n","                                                                 input_2[0][0]                    \n","__________________________________________________________________________________________________\n","input_3 (InputLayer)            (None, None, None, N 0                                            \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, None, None, N 0           multiply_2[0][0]                 \n","                                                                 input_3[0][0]                    \n","==================================================================================================\n","Total params: 2,327,041\n","Trainable params: 2,327,041\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 64, 64, 1)         0         \n","_________________________________________________________________\n","conv_s_n2d_1 (ConvSN2D)      (None, 64, 64, 64)        640       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv_s_n2d_2 (ConvSN2D)      (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","conv_s_n2d_3 (ConvSN2D)      (None, 32, 32, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv_s_n2d_4 (ConvSN2D)      (None, 16, 16, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","conv_s_n2d_5 (ConvSN2D)      (None, 16, 16, 256)       295168    \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv_s_n2d_6 (ConvSN2D)      (None, 8, 8, 256)         590080    \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 8, 8, 256)         1024      \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","conv_s_n2d_7 (ConvSN2D)      (None, 8, 8, 512)         1180160   \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 8, 8, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","conv_s_n2d_8 (ConvSN2D)      (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 4, 4, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 512)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 8192)              0         \n","_________________________________________________________________\n","dense_sn_1 (DenseSN)         (None, 1024)              8390656   \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 1024)              0         \n","_________________________________________________________________\n","dense_sn_2 (DenseSN)         (None, 1)                 1026      \n","=================================================================\n","Total params: 13,083,330\n","Trainable params: 13,076,673\n","Non-trainable params: 6,657\n","_________________________________________________________________\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            (None, 64, 64, 64, 1 0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            (None, 64, 64, 64, 1 0                                            \n","__________________________________________________________________________________________________\n","input_7 (InputLayer)            (None, 64, 64, 64, 1 0                                            \n","__________________________________________________________________________________________________\n","model_1 (Model)                 multiple             2327041     input_5[0][0]                    \n","                                                                 input_6[0][0]                    \n","                                                                 input_7[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 64, 64, 64, 1 0           model_1[1][0]                    \n","__________________________________________________________________________________________________\n","permute_1 (Permute)             (None, 64, 64, 64, 1 0           lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","permute_2 (Permute)             (None, 64, 64, 64, 1 0           lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 64, 64, 64, 1 0           lambda_1[0][0]                   \n","                                                                 permute_1[0][0]                  \n","                                                                 permute_2[0][0]                  \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 64, 64, 1)    0           concatenate_6[0][0]              \n","__________________________________________________________________________________________________\n","model_2 (Model)                 (None, 1)            13083330    lambda_2[0][0]                   \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 192, 1)       0           model_2[1][0]                    \n","==================================================================================================\n","Total params: 15,410,371\n","Trainable params: 2,327,041\n","Non-trainable params: 13,083,330\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"lqliyX4PR8qB"},"source":["# %% train\n","num_epochs = 20\n","l2_loss_train, l2_loss_test = [], []\n","gan_loss_train, gan_loss_test = [], []\n","d_loss_train, d_loss_test = [], []\n","\n","fnCp = 'HDnGAN_lw1e-3'\n","\n","total_train_num = clean_output_train.shape[0]\n","total_test_num = clean_output_test.shape[0]\n","print('Training on', total_train_num, 'blocks. Testing on', total_test_num, 'blocks.')\n","batch_size_train, batch_size_test = 1, 1\n","\n","for ii in range(num_epochs):\n","    cnt_train, cnt_test = 0, 0\n","    \n","    # shuffle the images\n","    index_train = np.arange(total_train_num)\n","    np.random.shuffle(index_train)\n","    noisy_block_train = noisy_block_train[index_train,:,:,:,:]\n","    bmask_block_train = bmask_block_train[index_train,:,:,:,:]\n","    clean_output_train = clean_output_train[index_train,:,:,:,:]\n","\n","    index_test = np.arange(total_test_num)\n","    np.random.shuffle(index_test)\n","    noisy_block_test = noisy_block_test[index_test,:,:,:,:]\n","    bmask_block_test = bmask_block_test[index_test,:,:,:,:]\n","    clean_output_test = clean_output_test[index_test,:,:,:,:]\n","\n","    print('----------------------------------------------------------------------')\n","    print('----------------------------------------------------------------------')\n","    print('----------------------------------------------------------------------')\n","    print('\\n')\n","    print('Total epoch count:', ii + 1)\n","    gan_loss_train_batch, l2_loss_train_batch, d_loss_train_batch = [], [], []\n","    gan_loss_test_batch, l2_loss_test_batch, d_loss_test_batch = [], [], []\n","    while cnt_train + batch_size_train < total_train_num:\n","        if cnt_test + batch_size_test >= total_test_num:\n","            cnt_test = 0\n","        \n","        print('\\n')\n","        print('Training blocks count:', cnt_train)\n","        \n","        # prepare training and testing batch\n","        train_batch_noisy = noisy_block_train[cnt_train:cnt_train+batch_size_train,:,:,:,:]\n","        train_batch_clean = clean_output_train[cnt_train:cnt_train+batch_size_train,:,:,:,:]\n","        train_batch_bmask = bmask_block_train[cnt_train:cnt_train+batch_size_train,:,:,:,:]\n","        train_batch_lmask = bmask_block_train[cnt_train:cnt_train+batch_size_train,:,:,:,:]\n","        \n","        test_batch_noisy = noisy_block_test[cnt_test:cnt_test+batch_size_test,:,:,:,:]\n","        test_batch_clean = clean_output_test[cnt_test:cnt_test+batch_size_test,:,:,:,:]\n","        test_batch_bmask = bmask_block_test[cnt_test:cnt_test+batch_size_test,:,:,:,:]\n","        test_batch_lmask = bmask_block_test[cnt_test:cnt_test+batch_size_test,:,:,:,:]\n","        \n","        # prepare labels and images for discriminator\n","        if ii == 0 and cnt_train == 0:\n","            generated_train = train_batch_noisy\n","            generated_test = test_batch_noisy\n","    \n","        else:\n","            generated_train = model_generator.predict([train_batch_noisy, train_batch_bmask, train_batch_lmask])[:,:,:,:,:input_ch_d]\n","            generated_test = model_generator.predict([test_batch_noisy, test_batch_bmask, test_batch_lmask])[:,:,:,:,:input_ch_d]\n","        \n","        generated_train_sag = generated_train\n","        generated_train_cor = np.transpose(generated_train,[0,2,1,3,4]) # generate images of different directions\n","        generated_train_axial = np.transpose(generated_train,[0,3,1,2,4])\n","        generated_test_sag = generated_test\n","        generated_test_cor = np.transpose(generated_test,[0,2,1,3,4])\n","        generated_test_axial = np.transpose(generated_test,[0,3,1,2,4])\n","        generated_train_all = np.concatenate((generated_train_sag, \n","                                                generated_train_cor, generated_train_axial), axis=0)\n","        generated_test_all = np.concatenate((generated_test_sag, \n","                                                generated_test_cor, generated_test_axial), axis=0)\n","        \n","        clean_train_sag = train_batch_clean\n","        clean_train_cor = np.transpose(train_batch_clean,[0,2,1,3,4]) \n","        clean_train_axial = np.transpose(train_batch_clean,[0,3,1,2,4])\n","        clean_test_sag = test_batch_clean\n","        clean_test_cor = np.transpose(test_batch_clean,[0,2,1,3,4])\n","        clean_test_axial = np.transpose(test_batch_clean,[0,3,1,2,4])\n","        clean_train_all = np.concatenate((clean_train_sag, clean_train_cor, clean_train_axial), axis=0)\n","        clean_test_all = np.concatenate((clean_test_sag, clean_test_cor, clean_test_axial), axis=0)\n","        \n","        generated_train, generated_test = generated_train_all, generated_test_all\n","        clean_train, clean_test = clean_train_all, clean_test_all\n","        \n","        shape_train = np.shape(generated_train)\n","        shape_test = np.shape(generated_test)\n","        generated_train = np.reshape(generated_train,[shape_train[0]*shape_train[1],shape_train[2],shape_train[3],1])\n","        generated_test = np.reshape(generated_test,[shape_test[0]*shape_test[1],shape_test[2],shape_test[3],1])\n","        clean_train = np.reshape(clean_train,[shape_train[0]*shape_train[1],shape_train[2],shape_train[3],2])[:,:,:,:input_ch_d]\n","        clean_test = np.reshape(clean_test,[shape_test[0]*shape_test[1],shape_test[2],shape_test[3],2])[:,:,:,:input_ch_d]\n","        \n","        dtrain_input_image_pred = np.zeros(1)\n","        dtrain_input_image_clean = np.zeros(1)\n","        flag = 1\n","        for jj in range(np.shape(generated_train)[0]):\n","            if np.sum(np.abs(clean_train[jj]) > 0 ) > 400: # discard empty and extremely sparse slices\n","                flag = 0\n","                if dtrain_input_image_pred.any():\n","                    dtrain_input_image_pred = np.concatenate((dtrain_input_image_pred, np.expand_dims(generated_train[jj],0)), axis=0)\n","                    dtrain_input_image_clean = np.concatenate((dtrain_input_image_clean, np.expand_dims(clean_train[jj],0)), axis=0)\n","                else:\n","                    dtrain_input_image_pred = np.expand_dims(generated_train[jj],0)\n","                    dtrain_input_image_clean = np.expand_dims(clean_train[jj],0)\n","        if flag: # empty block\n","            print('empty training block!')\n","            cnt_train += batch_size_train\n","            cnt_test += batch_size_test\n","            print('Total epoch: ', ii + 1)\n","            continue\n","    \n","        dtest_input_image_pred = np.zeros(1)\n","        dtest_input_image_clean = np.zeros(1)    \n","        flag = 1\n","        for jj in range(np.shape(generated_test)[0]):\n","            if np.sum(np.abs(clean_test[jj]) > 0 ) > 400:\n","                flag = 0\n","                if dtest_input_image_pred.any():\n","                    dtest_input_image_pred = np.concatenate((dtest_input_image_pred, np.expand_dims(generated_test[jj],0)), axis=0)\n","                    dtest_input_image_clean = np.concatenate((dtest_input_image_clean, np.expand_dims(clean_test[jj],0)), axis=0)\n","                else:\n","                    dtest_input_image_pred = np.expand_dims(generated_test[jj],0)\n","                    dtest_input_image_clean = np.expand_dims(clean_test[jj],0)\n","        if flag: \n","            print('empty testing block!')\n","            cnt_train += batch_size_train\n","            cnt_test += batch_size_test\n","            print('Total epoch: ', ii + 1)\n","            continue\n","        \n","        doutput_false_train_tag = np.zeros((1,np.shape(dtrain_input_image_pred)[0]))[0] \n","        doutput_true_train_tag = np.ones((1,np.shape(dtrain_input_image_clean)[0]))[0] \n","        doutput_false_test_tag = np.zeros((1,np.shape(dtest_input_image_pred)[0]))[0] \n","        doutput_true_test_tag = np.ones((1,np.shape(dtest_input_image_clean)[0]))[0] \n","        \n","        dtrain_input_image = np.concatenate((dtrain_input_image_pred, dtrain_input_image_clean), axis=0)\n","        dtrain_output_tag = np.concatenate((doutput_false_train_tag, doutput_true_train_tag), axis=0)\n","        dtest_input_image = np.concatenate((dtest_input_image_pred, dtest_input_image_clean), axis=0)\n","        dtest_output_tag = np.concatenate((doutput_false_test_tag, doutput_true_test_tag), axis=0)\n","    \n","        # train the discriminator\n","        print('----------------------------------------------------------------------')\n","        print('Training the discriminator')\n","        history1 = model_discriminator.fit(x = dtrain_input_image, \n","                                        y = dtrain_output_tag,\n","                                        validation_data = (dtest_input_image,\\\n","                                                                dtest_output_tag),\n","                                        batch_size = 10, \n","                                        epochs = 3,  \n","                                        shuffle = True, \n","                                        callbacks = None, \n","                                        verbose = 2)\n","    \n","        model_discriminator.trainable = False\n","        gtrain_output_tag = np.ones((batch_size_train, block_size*3, 1)) \n","        gtest_output_tag = np.ones((batch_size_test, block_size*3, 1)) \n","            \n","        # train the GAN\n","        print('----------------------------------------------------------------------')\n","        print('Training the GAN')\n","        \n","        history2 = model_gan.fit(x = [train_batch_noisy, train_batch_bmask, train_batch_lmask], \n","                                    y = [train_batch_clean, gtrain_output_tag],\n","                                    validation_data = ([test_batch_noisy, test_batch_bmask, test_batch_lmask], \\\n","                                                        [test_batch_clean, gtest_output_tag]),\n","                                    batch_size = 1, \n","                                    epochs = 1,  \n","                                    shuffle = True, \n","                                    callbacks = None, \n","                                    verbose = 2)\n","        \n","        l2_loss_train_batch.append(history2.history['model_1_loss'])\n","        gan_loss_train_batch.append(history2.history['lambda_3_loss'])\n","        d_loss_train_batch.append(history1.history['loss'])\n","        l2_loss_test_batch.append(history2.history['val_model_1_loss'])\n","        gan_loss_test_batch.append(history2.history['val_lambda_3_loss'])\n","        d_loss_test_batch.append(history1.history['val_loss'])\n","                                \n","        cnt_train += batch_size_train\n","        cnt_test += batch_size_test\n","        print('Total epoch: ', ii + 1)\n","        \n","    print('Discriminator loss: train:',np.mean(d_loss_train_batch),'test:', np.mean(d_loss_test_batch))\n","    print('GAN loss: train:',np.mean(gan_loss_train_batch),'test:', np.mean(gan_loss_test_batch))\n","    print('L2 loss: train:',np.mean(l2_loss_train_batch),'test:', np.mean(l2_loss_test_batch))\n","    d_loss_train.append(np.mean(d_loss_train_batch))\n","    d_loss_test.append(np.mean(d_loss_test_batch))\n","    gan_loss_train.append(np.mean(gan_loss_train_batch))\n","    gan_loss_test.append(np.mean(gan_loss_test_batch))\n","    l2_loss_train.append(np.mean(l2_loss_train_batch))\n","    l2_loss_test.append(np.mean(l2_loss_test_batch))\n","    \n","    \n","    fpCp1 = os.path.join(dpRoot, 'discriminator', fnCp + '_epoch' + str(ii + 1) + '.h5')\n","    fpCp2 = os.path.join(dpRoot, 'generator', fnCp + '_epoch' + str(ii + 1) + '.h5')\n","    fpLoss = os.path.join(dpRoot, 'loss', fnCp + '_loss.mat') \n","    model_discriminator.save(fpCp1)\n","    model_generator.save(fpCp2)\n","    sio.savemat(fpLoss, {'l2_loss_train':l2_loss_train, 'l2_loss_test': l2_loss_test,\n","                        'gan_loss_train': gan_loss_train, 'gan_loss_test': gan_loss_test,\n","                        'd_loss_train': d_loss_train, 'd_loss_test': d_loss_test})\n","print('Training finished')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85QIZa2go3t7"},"source":["# %% apply \n","print('Applying...')\n","clean_block_pred = []\n","for ii in range(len(noisy_block_apply)):\n","    noisy_block_tmp = np.expand_dims(noisy_block_apply[ii], 0)\n","    bmask_block_tmp = np.expand_dims(bmask_block_apply[ii], 0)\n","    clean_block_pred_tmp = np.squeeze(model_generator.predict([noisy_block_tmp, bmask_block_tmp, bmask_block_tmp])[:, :, :, :, 0])\n","    clean_block_pred.append(clean_block_pred_tmp)\n","clean_pred, vol_count = utils.block2brain(np.expand_dims(np.array(clean_block_pred), -1), ind_apply, bmask_apply)\n","fpPred = os.path.join(dpSub_apply, 'pred', fnCp +'_pred.nii.gz')\n","utils.save_nii(fpPred, clean_pred, fpClean)\n","print('Applying finished')"],"execution_count":null,"outputs":[]}]}